# EM Algorithm

## Introduction

The EM algorithm is an application of the MM algorithm.
Proposed by @Demp:Lair:Rubi:maxi:1977, it is one of the 
pillars of modern computational statistics.
Every EM algorithm has some notion of missing data.

Setup:

- Complete data $X = (Y, Z)$, with density $f(x | \theta)$.
- Observed data $Y$. Some function $t(X) = Y$ collapses $X$ into $Y$.
- Missing data $Z$.

The definition of $X$ is left up to the creativity of the statistician.
The general idea is to choose $X$ so that the MLE is trivial for complete data.


## EM Algorithm
The EM algorithm iterates between the two steps until convergence:

1. E-step: compute the conditional expectation 
  \begin{equation*}
  Q(\theta | \theta_n) = E [\log f(X | \theta) | Y = y, \theta_n],
  \end{equation*}
   where $\theta_n$ is the current estimate of $\theta$.
   Note that expectation needed is for functions of missing data $Z$ 
   instead of $Z$ itself, although sometimes it can be $Z$ itself.

1. M-step: maximize $Q(\theta | \theta_n)$ with respect to $\theta$
   to obtain the new estimate $\theta_{n + 1}$.

Ascent property:
Let $g(y | \theta)$ be the observed likelihood. Then the EM algorithm
enjoys the ascent property:
\begin{equation*}
  \log g(y | \theta_{n + 1}) \ge \log g(y | \theta_n).
\end{equation*}

It is sufficient to show the minorization inequality:
\begin{equation*}
  \log g(y | \theta) \ge Q(\theta | \theta_n) + \log g(y | \theta_n) - Q(\theta_n | \theta_n).
\end{equation*}


Information inequality:
For densities $h_1$ and $h_2$ with respect to measure $\mu$,
$E_{h_1} \log h_1(X) \ge E_{h_1} \log h_2(X)$
with equality only if $h_1 = h_2$ almost everywhere relative to $\mu$.

To prove the ascent property,
let $h_1(x | y, \theta) = f(x|\theta) / g(y | \theta)$ and 
$h_2(x | y, \theta_n) = f(x | \theta_n) / g(y | \theta_n)$ 
be conditional densities of $X$ on the set $\{x: t(x) = y\}$ 
with respect to some measure $\mu_y$.


## Example: Clustering by EM

Suppose that $y_1, \ldots, y_n$ form a random sample from 
a mixture density $h(y) = \sum_{j=1}^k \pi_j h_j(y | \theta)$,
where $\pi_j > 0$, $j = 1, \ldots, n$, and $\sum_{j=1}^k \pi_j = 1$,
$h_j$ is the density function of group $j$ with parameter $\theta$.
For example, each group density can be normal with common variance
$\sigma^2$ but with group specific mean $\mu_j$'s.

Problem: estimate the parameters $\pi_j$'s and $\theta$.

Missing data: let $z_{ij}$ be the indicator such that
$z_{ij} = 1$ if observation $y_i$ is from group $j$, and zero otherwise.

Complete data likelihood:
\[
  \sum_{i=1}^n \sum_{j=1}^k z_{ij}[\log \pi_j + \log h_j(y_i | \theta)].
\]



E-step: conditional expectation of $z_{ij}$, $w_{ij}$, given current
$\pi_j$'s and $\theta$.
By Bayes' rule, 
\begin{equation*}
  w_{ij} = \frac{\pi_j h_j(y_i | \theta)}{\sum_{l=1}^k \pi_l h_l(y_i | \theta)}
\end{equation*}


M-step: 
with missing values filled by their conditional expectations $w_{ij}$,
the maximization step separates $\pi$ from $\theta$.
\begin{itemize}
\item 
For $\pi_j$'s, maximize $\sum_{j=1}^k s_j \log \pi_j$,
where $s_j = \sum_{l=1}^n w_{ij}$.
Closed-form solution
$\hat\pi_j = s_j / m$.
\item
For $\theta$, maximize
\begin{equation*}
  \sum_{i=1}^n \sum_{j=1}^k w_{ij}\left[ - \frac{1}{2}\log \sigma^2 - \frac{(y_i - \mu_j)^2}{2 \sigma^2}\right].
\end{equation*}
Closed-form solution:
\begin{equation*}
\hat\mu_j = \frac{\sum_{i=1}^n w_{ij} y_i}{\sum_{i=1}^n w_{ij}},
\end{equation*}
and
\begin{equation*}
\hat\sigma^2 = \frac{\sum_{i=1}^n \sum_{l=1}^k w_{ij}(y_i - \mu_j)^2}{n}.  
\end{equation*}
\end{itemize}



Question: what if there is no closed form E-step or M-step?

## Variants of EM
### MCEM
Classroom examples may give an impression that the E-step consists
of replacing the missing data by their conditional expectations 
given the observed data at current parameter values. 
Although in many examples this may be the case as the complete
loglikelihood is a linear function of the missing data $Z$, 
it is not quite so in general. 


When the E-step has no closed-form, it can be approximated
by a Monte Carlo process, and this variant of the EM algorithm 
is known as the Monte Carlo EM (MCEM) @Wei:Tann:mont:1990.
The Monte Carlo E-step goes as the following.
\begin{enumerate}
\item 
Draw a random sample of size $M_n$ of the missing values $Z$,
$z_1, \ldots, z_{M_n}$, from the conditional distribution 
of $Z$ given $Y$ denoted by $k(z | y; \theta_n)$ at the 
current parameter value $\theta_n$.
\item
Approximate $Q(\theta | \theta_n)$ by 
\[
\sum_{i=1}^{M_n} \frac{1}{M_n} \log f\big((Y, z_i) | \theta_n\big).
\]
\end{enumerate}
The sample size can be different from iteration to iteration; 
smaller sample size may by sufficient earlier on but larger 
sample sizes are needed in the vicinity of the
convergence to control the Monte Carlo error.
Importance sampling can be used as well.


MCEM routines need to address two challenges @Lavi:Case:impl:2001
(1) how do we minimize the computational cost in obtaining an 
sample? and (2) how do we choose the Monte Carlo sample size?
Rejection sampling and importance sampling can be used for the first.
For the second, the number of simulations at iterations in which the 
change in the parameter value is swamped by Monte Carlo error needs to
be increased in an automated way.


### ECM
The Expectation Conditional Maximization (ECM) algorithm
@Meng:Rubi:maxi:1993 is a class of generalized 
EM (GEM) algorithms [@Demp:Lair:Rubi:maxi:1977], where the M-step
is only partially implemented, with the new estimate improving the
likelihood found in the E-step, but not necessarily maximizing it.


The ECM algorithm replaces the M-step of the EM algorithm with several
computationally simpler conditional maximization (CM) steps.
Each of these CM-steps maximizes the $Q$-function found in the preceding 
E-step subject to constraints on $\theta$, where the collection of all 
constraints is such that the maximization is over the full parameter 
space of $\theta$. It is essentially coordinate ascend!


A CM-step might be in closed form or it might itself require iteration, but because the CM maximizations are over smaller dimensional spaces, often they are simpler, faster, and more stable than the corresponding full maximizations called for on the M-step of the EM algorithm, especially when iteration is required. The ECM algorithm typically converges more slowly than the EM in terms of number of iterations, but can be faster in total computer time. More importantly, the ECM  algorithm preserves the appealing convergence properties of the EM algorithm, such as its monotone convergence. 
<!-- % http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/ebooks/html/csa/node46.html#SECTION08141000000000000000 -->


## Standard Errors

<!-- % Various methods have been proposed -->
<!-- % (Louis 1982; Meilijson 1989; Meng and Rubin 1993, and Lange 1995). -->
<!-- % We focus on Oakes (1999) and its Monte Carlo version, which  -->
<!-- % is both simple and useful. -->

### Supplemental EM (SEM)
@Meng:Rubi:usin:1991 proposed a general automated algorithm
named SEM to obtain numerically stable asymptotic variance matrix
of the estimator from the EM algorithm.
The method uses the fact that the rate of convergence of EM
is governed by the fractions of the missing information to
find the increased variability due to missing information to
add to the complete-data variance matrix.


Keeping the notation of $X = (Y, Z)$, from the factorization 
\[
f(X | \theta) = g(Y | \theta) k(Z | Y; \theta),
\]
where $f$, $g$, and $k$ are the joint, marginal and 
conditional density of their arguments, respectively.
The observed loglikelihood is the difference between the 
complete loglikelihood and the conditional loglikelihood
\[
L(\theta | Y) = L(\theta | X) - \log k(Z | Y; \theta).
\]
Taking the second derivatives, averaging over 
$k(Z | Y; \theta)$, and evaluate at the MLE $\theta = \theta^*$,
we have
\[
I_o(\theta^* | Y) = I_{oc} - I_{om},
\]
where 
\[
I_o(\theta | Y) = - \frac{\partial^2 \log g(Y | \theta)}{\partial\theta\partial\theta^{\top}}
\]
is the observed information matrix,
\[
I_{oc} = E[I_o(\theta | Y) \vert Y, \theta] \big\vert_{\theta = \theta^*}
\]
the conditional expectation of the complete-data observed information
given the observed data, and
\[
I_{om} = E\left[ - \frac{\partial^2 \log k(Z | Y; \theta)}{\partial\theta\partial\theta^2} \big\vert Y, \theta\right] \big\vert_{\theta = \theta^*}
\]
is viewed as the missing information.
The interpretation is appealing:
\[
\mbox{observed information} = \mbox{complete information} - \mbox{missing information}
\]
which is known as the ``missing information principal''.


The observed information can be written as
\[
I_o(\theta^* | Y) = (I - R) I_{oc},
\]
where $R =  I_{om} I_{oc}^{-1}$.
It has been shown that the convergence rate of the EM 
algorithm is $R$ [@Demp:Lair:Rubi:maxi:1977].
If an estimate of $R$ is available, the target variance matrix
can be estimated using 
\[
I_{oc}^{-1} (I - R)^{-1} = I_{oc}^{-1} + I_{oc}^{-1}R (I - R)^{-1}.
\]


The SEM algorithm needs to evaluate $I_{oc}^{-1}$ and $R$.
Evaluation of $I_{oc}$ is simplified if $X$ is from an exponential family.
It can be obtained simply by substituting the conditional expectation
of the sufficient statistics $S(Y)$ found at the last E-step.
Non-exponential family cases can be handled by linearization
of the complete loglikelihood in terms of $S(Y)$.


The computation of $R$ is done numerically.
Each element of $R$ is estimated by the component-wise
rate of convergence of a ``forced EM''.
Let $r_{ij}$ be the $(i,j)$th element of $R$.
Let 
\[
\theta^{(t)}(i) = (\theta_1^*, \ldots, \theta_{i-1}^*, \theta_i^{(t)}, \theta_{i+1}^*, \ldots, \theta_d^*),
\]
that is, only the $i$th component in the $d$-dimensional vector
$\theta^{t}(i)$ is active in the sense that other components are
fixed at their MLE's.
Given $\theta^*$ and $\theta^{(t)}$, one iteration of the EM
algorithm can be run to obtain 
$\tilde\theta^{(t+1)}(i) = M\big(\theta^{(t)}(i)\big)$.
Then, obtain
\[
r_{ij}^{(t)} = \frac{\tilde\theta_j^{(t+1)}(i) - \theta_j^*}{\theta_i^{(t)} - \theta_i^*},
\]
for $j = 1, \ldots, d$.
This rate approximates the slope of the map of
$\theta^{(t + 1)} = M(\theta^{(t)})$.


### Direct Calculation of the Information Matrix

@Oake:dire:1999 derived an explicit formula for the 
observed information matrix in terms of derivatives of the
$Q$ function (conditional expectation of the complete-data
loglikelihood given the observed data) invoked by the EM algorithm.


Start from the fundamental identity:
\begin{equation}
L(\phi, X) = Q(\phi' | \phi) - E_{X\mid Y,\phi} \log k(X \mid Y; \phi')
(\#eq:infoiden)
\end{equation}
where $Q(\phi' \mid \phi) = E_{X \mid Y,\phi} L_0(\phi', X)$, and
$L_0$ is the complete-data loglikelihood.


Assuming that the usual exchange of expectation with respect to
$X$ and differentiation in $\phi$ hold for $\log k(X | Y, \phi)$.
This gives two identities
\[
E_{X\mid Y,\phi} \frac{\partial\log k(X\mid Y; \phi)}{\partial \phi} = 0
\]
and
\[
E_{X\mid Y,\phi} \frac{\partial^2 \log k(X\mid Y; \phi)}{\partial\phi^2}
+ E_{X\mid Y,\phi} \frac{\partial \log k(X\mid Y; \phi)}{\partial \phi} 
\left[\frac{\partial \log k(X\mid Y; \phi)}{\partial \phi}\right]^{\top}.
\]

Differentiation of \@ref(eq:infoiden) in $\phi'$ gives
\begin{equation}
  \frac{\partial L}{\partial \phi} = 
  \frac{\partial Q(\phi' | \phi)}{\partial \phi'}
  - E_{X\mid Y, \phi} \frac{\partial \log k(X\mid Y, \phi')}{\partial \phi'} .
  (\#eq:dinfo)
\end{equation}
Substituting $\phi$ with $\phi'$ makes the second term vanish,
which leads to 
\[
\frac{\partial L}{\partial \phi} = 
\left\{\frac{\partial Q(\phi' | \phi)}{\partial \phi'} \right\}_{\phi = \phi'}.
\]


Differentiation of \@ref(eq:dinfo) in $\phi'$ and $\phi$ gives
respectively
\[
\frac{\partial^2 L}{\partial \phi'^2} = \frac{\partial^2 L}{\partial \phi'^2}
- E_{X\mid Y, \phi} \frac{\partial^2 \log k(X\mid Y, \phi')}{\partial \phi'^2},
\]
and
\[
0 = \frac{\partial^2 Q(\phi'|\phi)}{\partial \phi'\partial \phi}
- E_{X\mid Y, \phi} \frac{\partial \log k(X\mid Y, \phi')}{\partial \phi'} 
\left[\frac{\partial \log k(X\mid Y, \phi)}{\partial \phi} \right]^{\top}.
\]
Substituting $\phi = \phi'$, adding the two equations and using
the information identity give
\[
\frac{\partial^2 L}{\partial \phi^2} = 
\left\{\frac{\partial^2 Q(\phi'|\phi)}{\partial \phi'^2}
  + \frac{\partial^2 Q(\phi'|\phi)}{\partial \phi'\partial \phi}
  \right\}_{\phi' = \phi},
\]
which is valid for all $\phi$.
The second term is the ``missing information'' due to the
fact that only $Y$ instead of $X$ is observed.


If the complete data is from an exponential family, the 
second term reflects the sensitivity of the imputed complete-data
sufficient statistic to changes in hypothesized parameter value.


<!-- %%%% DON'T remove yet; need to find its source! -->
<!-- % The second derivative can be expressed in terms of complete-data -->
<!-- % likelihood: -->
<!-- % \begin{equation*} -->
<!-- %   \frac{\partial^2}{\partial\theta^2} \log (\theta | \mathbf{x}) -->
<!-- %   = \left\{ \frac{\partial^2}{\partial\theta'^2} E \log L(\theta'| \mathbf{x}, \mathbf{z}) + \frac{\partial^2}{\partial\theta'\partial\theta} E  \log L(\theta'| \mathbf{x}, \mathbf{z}) \right\} \big\vert_{\theta' = \theta}   -->
<!-- % \end{equation*} -->
<!-- % where the expectation is taken with respect to the missing data. -->


<!-- % Take the derivatives inside the expectations: -->
<!-- % \begin{equation*} -->
<!-- %   E\left( \frac{\partial^2}{\partial\theta^2} \log L(\theta | \mathbf{x}, \mathbf{z})\right) + \VAR\left( \frac{\partial}{\partial\theta} \log L(\theta | \mathbf{x}, \mathbf{z}) \right) -->
<!-- % \end{equation*} -->
<!-- % which allows Monte Carlo evaluation. -->

## Acceleration

Can we obtain fast convergence without sacrificing the stability of EM?



## Example: Hidden Markov Model
A hidden Markov model (HMM) is a dependent mixture model.
The model consists of two parts:
1) a Markov process for the unobserved state process
$\{S_t: t = 1, 2, \ldots\}$; and
2) a state-dependent process for the observed data 
$\{X_t: t = 1, 2, \ldots\}$ such that when $S_t$ is known,
the distribution of $X_t$ depends on $S_t$ only and not
on previous states or observations.
It can be characterized by
\begin{align}
  \Pr(S_t \mid S_1, \ldots, S_{t - 1}) &= \Pr(S_t \mid S_{t-1}),\quad t = 2, 3, \ldots, \\
  \Pr(X_t \mid S_1, \ldots, S_t; X_1, \ldots, X_{t-1}) &= \Pr(X_t \mid S_t), \quad t = 1, 2, \ldots. 
\end{align}
HMMs have a wide range of applications. 


Suppose that process $S_t$ has $m$ states.
Let $\delta$ be the parameters of the initial distribution of $S_t$.
Let $\Gamma$ be the probability transition matrix of $S_t$.
Let $\theta$ be the parameter vector in $\Pr(X_t | S_t)$.
Given the observed data $\{x_t: t = 1, \ldots, T\}$, 
these parameters can be  estimated by maximizing the 
an EM algorithm that treats the unobserved states 
$\{s_t: t = 1, \ldots, T\}$ as missing data.


To devise the EM algorithm, we need the complete data loglikelihood.
Let $u_j(t) = I(s_t = j)$, $t = 1, \ldots, T$, $j = 1, \ldots, m$,
and $v_{jk}(t) = I(s_{t-1} = j, s_t = k)$, $t = 2, \ldots, T$,
and $j, k \in \{1, \ldots, m\}$.
The complete data loglikelihood is
\begin{align*}
 & \log p(s_1, \ldots, s_T, x_1, \ldots, x_T)\\
 =& \log \delta_{s_1} \prod_{t=2}^T \Gamma_{s_{t-1}, s_t} \prod_{t=1}^T p(x_t | s_t; \theta)\\
 =& \log \delta_{s_1} + \sum_{t=2}^T \log \Gamma_{s_{t-1}, s_t} + \sum_{t=1}^T \log p(x_t | s_t; \theta)\\
 =& \sum_{i=1}^m u_j(1) \log \delta_j 
   + \sum_{j=1}^m \sum_{k=1}^m \sum_{t=2}^T v_{jk}(t) \log \Gamma_{jk}
   + \sum_{j=1}^m \sum_{t=1}^T u_j(t) \log p(x_t | s_t = j; \theta)
\end{align*}


In the E-step, quantities $u_j(t)$ and $v_{jk}(t)$ needs be replaced with
their conditional expectations given $\{x_t: t = 1, \ldots, T\}$.
Define row vector $\alpha(t)$ as
\[
\alpha(t) = \delta \prod_{r=2}^t \Gamma P(x_r),
\]
where $P(x_r | \theta) = diag[p(x_r | 1; \theta), \ldots, p(x_r | m; \theta)]$.
This is indeed a probability vector, known as the forward probability,
because the $j$th component is $\alpha_j(t) = p(x_1, \ldots, x_t, s_t = j)$.
Define vector $\beta(t)$ as
\[
\beta(t) = \prod_{r = t + 1}^T \Gamma P(x_r) \mathbf{1},
\]
with the convention that an empty product is the identity matrix,
where $\mathbf{1}$ is the column vector of ones.
This is also a probability vector, known as the backward probability,
because the $j$th component is
$\beta_j(t) = p(x_{t+1}, \ldots, x_T | s_t = j)$.
The forward probabilities are affected by the initial probabilities 
$\delta$, and the model does not need to assume stationarity of $S_t$.
The backward probabilities, however, are not affected by stationarity 
of $S_t$.


It follows that for all $t \in \{1, \ldots, T\}$ and
$j \in \{1, \ldots, m\}$,
\[
\alpha_j(t) \beta_j(t) = p(x_1, \ldots, x_T, s_t = j).
\]
Consequently, $\alpha(t) \beta(t) = p(x_1, \ldots, x_T) = L_T$ for each $t$.
It can be verified that, for $t \in \{1, \ldots, T\}$,
\begin{equation}
p(S_t = j | x_1, \ldots, x_T) = \alpha_j(t) \beta_j(t)  / L_T,
(\#eq:condexu)
\end{equation}
and that, for $t \in \{2, \ldots, T\}$,
\begin{equation}
p(s_{t - 1} = j, s_t = k | x_1, \ldots, x_T) 
= \alpha_j(t - 1) \Gamma_{jk} p(x_t | k; \theta) \beta_k(t) / L_T.
(\#eq:condexv)
\end{equation}

__equation labels cannot contain underscore__

Equations \@ref(eq:condexu) and \@ref(eq:condexv), respectively,
give the needed conditional expectation in the E-step for $u_j(t)$ 
and $v_{jk}(t)$ given the observations $\{x_t: t = 1, \ldots, T\}$.


In the M-step, the maximization with respect to three sets of 
parameters $\delta$, $\Gamma$, and $\theta$ can be done separately; 
each term in the summation depends only on one set.
Maximization with respect to $\delta$ and $\Gamma$ has closed-form
solution, while maximization with respect to $\theta$ needs to be
done numerically in general.


## Exercises

### Finite mixture regression
Given $n$ independent observations of the response $Y \in \mathbb{R}$ and
predictor $\mathbf{X} \in \mathbb{R}^p$, multiple linear regression models are
commonly used to explore the conditional mean structure of $Y$ given
$\mathbf{X}$. However, in many applications, the underlying assumption that the
regression relationship is homogeneous across all the observations
$(y_1, \mathbf{x}_1),\ldots,(y_n, \mathbf{x}_n)$
can be easily violated. Instead, the observations may form several distinct clusters indicating mixed relationships between the response and the predictors. Such heterogeneity can be more appropriately
modeled by a __finite mixture regression model__, consisting of, say, $m$
homogeneous groups/components.


Suppose the density of $y_i$ (conditional on $\mathbf{x}_i$), is given by
\begin{eqnarray}
f(y_i\mid \mathbf{x}_i,\boldsymbol{\Psi})= \sum_{j=1}^{m} \pi_{j}\phi(y_i;\mathbf{x}_i^{\top}\boldsymbol{\beta}_{j}, \sigma^2),\qquad i=1,\ldots,n,
(\#eq:mixregequal)
\end{eqnarray}
where $\pi_j$s are called mixing proportions, $\boldsymbol{\beta}_j$ is the regression coefficient vector for the $j$th group, $\phi(\cdot\:;\mu,\sigma^2)$ denotes the density function of $N(\mu,\sigma^2)$, and $\boldsymbol{\Psi}=(\pi_1,\boldsymbol{\beta}_1,\ldots,\pi_m,\boldsymbol{\beta}_m,\sigma)^T$ collects all the unknown parameters.


Maximum likelihood estimation is commonly used to infer the unknown arameter
$\boldsymbol{\Psi}$ in \@ref(eq:mixregequal), i.e.,
\begin{equation}
\hat{\bPsi}_{\mbox{mle}}=\arg\max_{\bPsi}\sum_{i=1}^n\log\left\{\sum_{j=1}^m\pi_j\phi(y_i;\bx_i^{\top}\bbeta_{j},\sigma^2)\right\}.
(\#eq:mixlinloglh)
\end{equation}
The MLE does not have an explicit form and the problem is usually solved by the EM algorithm.

Let $z_{ij} = 1$ if $i$th observation is from $j$th component} and zero otherwise. Denote the complete data by $\{(\mathbf{x}_{i}, \mathbf{z}_{i}, y_{i}); i =1,2,\ldots,n\}$, where the component labels
$\mathbf{z}_{i} = (z_{i1}, z_{i2}, \ldots, z_{im})$ are not observable or "missing" in practice. The complete log-likelihood can be written as
\[
l_{n}^{c}(\boldsymbol{\Psi})=\sum_{i=1}^{n}\sum_{j=1}^{m}z_{ij}\log \left\{\pi_{j}\phi(y_{i}-\mathbf{x}_{i}^{\top}\boldsymbol{\beta}_{j};0,\sigma^{2})\right\}.
\]
In the E-step, we calculate the conditional expectation of the complete
log-likelihood with respect to $\mathbf{z}_{i}$, and in the M-step, we maximize the obtained conditional expectation with respect to $\bPsi$. 


At the $k$th iteration, the E-Step computes the conditional expectation of $l_{n}^{c}(\boldsymbol{\Psi})$:
\begin{align*}
Q(\bPsi\mid \bPsi^{(k)}) =\sum_{i=1}^{n}\sum_{j=1}^{m}p_{ij}^{(k+1)} \left\{\log\pi_{j}+\log\phi(y_{i}-\bx_{i}^{\top}\bbeta_{j};0,\sigma^{2})\right\},
\end{align*}
where
\begin{align*}
p_{ij}^{(k+1)}&=E(z_{ij}\mid y_{i},\bx_{i};\bPsi^{(k)})
=\frac{\pi_{j}^{(k)}\phi(y_{i}-\bx_{i}^{\top}\bbeta_{j}^{(k)};0,\sigma^{2^{(k)}})}{\sum_{j=1}^{m}\pi_{j}^{(k)}\phi(y_{i}-\bx_{i}^{\top}\bbeta_{j}^{(k)};0,\sigma^{2^{(k)}})}.
\end{align*}
The M-Step maximizes $Q(\boldsymbol{\Psi} \mid \boldsymbol{\Psi}^{(k)})$ to obtain 
\begin{align*}
   \pi_{j}^{(k+1)} &=\frac{\sum_{i=1}^{n}p_{ij}^{(k+1)}}{n}\\
    \boldsymbol{\beta}_{j}^{(k+1)}&=\left(\sum_{i=1}^{n}\bx_i\bx_{i}^{\top}p_{ij}^{(k+1)}\right)^{-1}\left(\sum_{i=1}^{n}\bx_ip_{ij}^{(k+1)}y_i\right),\qquad j=1,\ldots,m;\\
    \sigma^{2^{(k+1)}}&=\frac{\sum_{j=1}^{m}\sum_{i=1}^{n}p_{ij}^{(k+1)}(y_{i}-\bx_{i}^{\top}\boldsymbol{\beta}_{j}^{(k+1)})^{2}}{n}.
\end{align*}

1. Follow the lecture notes to verify the validity of the provided E- and M-steps. That is, derive the updating rules in the given algorithm based on
the construction of an EM algorithm.

1. Implement this algorithm in R with a function `regmix_em`.
The inputs of the functions are
`y` for the response vector,
`xmat` for the design matrix,
`pi.init` for initial values of $\pi_j$'s (a vector of $K\times
1$ vector),
`beta.init` for initial values of $\boldsymbol{\beta}_j$'s (a matrix of
$p \times K$ where $p$ is `ncol(xmat)` and $K$ is the number
of components in the mixture),
`sigma.init` for initial values of $\sigma$,
and a `control` list for controlling max iteration number and
convergence tolerance.
The output of this function is the EM estimate of all the parameters.

1. Here is a function to generate data from the mixture regression model.
```{r regmix_sim}
regmix_sim <- function(n, pi, beta, sigma) {
    K <- ncol(beta)
    p <- NROW(beta)
    xmat <- matrix(rnorm(n * p), n, p) # normal covaraites
    error <- matrix(rnorm(n * K, sd = sigma), n, K)
    ymat <- xmat %*% beta + error # n by K matrix
    ind <- t(rmultinom(n, size = 1, prob = pi))
    y <- rowSums(ymat * ind)
    data.frame(y, xmat)
}
```
Generate data with the following and estimate the parameters.
```{r sim}
n <- 400
pi <- c(.3, .4, .3)
bet <- matrix(c( 1,  1,  1, 
                -1, -1, -1), 2, 3)
sig <- 1
set.seed(1205)
dat <- regmix_sim(n, pi, bet, sig)
## regmix_em(y = dat[,1], xmat = dat[,-1], 
##           pi.init = pi / pi / length(pi),
##           beta.init = matrix(rnorm(6), 2, 3),
##           sigma.init = sig / sig, 
##           control = list(maxit = 500, tol = 1e-5))
```

### A Poisson-HMM for earthquake data
Consider a $m$-state HMM where the observed data follows 
a Poisson distribution with mean $\lambda_i$ for state $i$, 
$i = 1, \ldots, m$, respectively.
This model has three sets of parameters
initial distribution $\delta$, transition probability matrix $\Gamma$,
and Poisson means $\lambda = (\lambda_1, \ldots, \lambda_m)$.
Suppose the observed data is a vector $x$.
Write a function \texttt{poisson.hmm.em} that implements the EM
algorithm for this model with these argument:

- `x`: the observed data;
- `m`: the number of states;
- `lambda`: initial value of $\lambda$;
- `Gamma`: initial value of $\Gamma$;
- `delta`: initial value of $\delta$;
- `control`: a named list similar to \texttt{glm.control}
that specifies the tolerance, maximum number of iteration, and 
whether or not trace the iteration.

Apply the function to model the frequency of major earthquakes
(magnitude 7 or above) in the world from 1900 to 2015 with a
2-state HMM and a 3-state HMM.
```{r quake}
## frequency of major earthquake in the world from 1900 to 2015
## raw data accessed at http://earthquake.usgs.gov/earthquakes/search/
qk7freq <- c(3, 2, 4, 1, 2, 5, 8, 3, 2, 5,
             5, 7, 3, 4, 6, 4, 8, 5, 12, 8, 
             7, 9, 7, 12, 9, 12, 13, 11, 16, 15, 
             9, 19, 9, 8, 12, 14, 11, 9, 21, 14, 
             7, 13, 11, 18, 13, 5, 10, 13, 11, 9, 
             13, 11, 7, 9, 6, 10, 8, 21, 8, 8, 
             13, 12, 10, 17, 12, 18, 9, 11, 22, 14, 
             17, 20, 16, 9, 11, 13, 14, 10, 12, 8, 
             6, 10, 7, 14, 14, 15, 11, 13, 11,  9, 
             18, 17, 13, 12, 13, 20, 15, 16, 12, 18, 
             15, 16, 13, 15, 16, 11, 11, 18, 12, 17, 
             24, 20, 16, 19, 12, 19)


## forward/backward probability for Poisson-HMM from 
## Zucchini and MacDonald (2009): 
## Hidden Markov Models for Time Series: An Introduction with R.
pois.HMM.lalphabeta <- function(x, m, lambda, gamma, delta = NULL) {
    if (is.null(delta)) 
        delta <- solve(t(diag(m) - gamma + 1), rep(1, m))
    n <- length(x)
    lalpha <- lbeta <- matrix(NA, m, n)
    allprobs <- outer(x, lambda, dpois)
    foo <- delta * allprobs[1, ]
    sumfoo <- sum(foo)
    lscale <- log(sumfoo)
    foo <- foo/sumfoo
    lalpha[, 1] <- log(foo) + lscale
    for (i in 2:n) {
        foo <- foo %*% gamma * allprobs[i, ]
        sumfoo <- sum(foo)
        lscale <- lscale + log(sumfoo)
        foo <- foo/sumfoo
        lalpha[, i] <- log(foo) + lscale
    }
    lbeta[, n] <- rep(0, m)
    foo <- rep(1/m, m)
    lscale <- log(m)
    for (i in (n - 1):1) {
        foo <- gamma %*% (allprobs[i + 1, ] * foo)
        lbeta[, i] <- log(foo) + lscale
        sumfoo <- sum(foo)
        foo <- foo/sumfoo
        lscale <- lscale + log(sumfoo)
    }
    list(la = lalpha, lb = lbeta)
}
```
