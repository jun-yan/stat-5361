# Monte Carlo Integration {#mcinteg}

## Particle Filtering for State Space Models

Consider a simple state space model
\begin{eqnarray*}
Y_{t}  \mid & X_t,    & \theta & \sim & N(X_t,     & \sigma^2)\\
X_{t}  \mid & X_{t-1},& \theta & \sim & N(X_{t-1}, & \tau^2)
\end{eqnarray*}
where $X_0 \sim N(\mu_0, \sigma^2_0)$ and $\theta = (\sigma^2, \tau^2)$.
This model is also known as a dynamic linear model. The observed data are $Y_1, \ldots, Y_T$. We would like to predict the hidden states $X_1, \ldots, X_T$. For this simple normal model, closed form solutions exist but we use SIS for illustration.


First, let's generate data.
```{r dlmSim}
dlmSim <- function(n, sigma, tau, x0) {
  x    <- rep(0, n)
  y    <- rep(0, n)
  x[1] <- rnorm(1, x0,   tau)
  y[1] <- rnorm(1, x[1], sigma)
  for (t in 2:n) { # loop for clarity; could be vectorized
    x[t] <- rnorm(1, x[t-1], tau)
    y[t] <- rnorm(1, x[t],   sigma)
  }
  data.frame(y = y, x = x) # save x for comparison purpose
}
```
Let $\tau^2 = 0.5$ and $\sigma^2 = 2\tau^2$.
Generate a series of length $n = 50$.
```{r}
n <- 50
tau <- sqrt(0.5); sigma <- sqrt(2) * tau
set.seed(123456)
dat <- dlmSim(n, sigma, tau, 0)
```

Now we use apply SIS to make inferences about $X_1, \ldots, X_T$.
```{r dlmSIS}
dlmSIS <- function(y, sigma, tau, m0, s0, N) {
  xseq <- matrix(0, n, N)
  x <- rnorm(N, m0, s0)
  w <- rep(1/N, N)
  for (t in 1:n) {
    x <- rnorm(N, x, tau)
    w <- w * dnorm(y[t], x, sigma)
    xseq[t, ] <- x <- sample(x, size = N, replace = TRUE, prob = w)
  }
  xseq
}
```
We run this SIS with $\mu_0 = 0$ and $\sigma_0 = 10$ with $N = 1000$ particles. It is easy to see that the particle sample degenerates.
```{r dlmSIS-Run}
xseq <- dlmSIS(dat$y, sigma, tau, 0, 10, 1000)
summary(apply(xseq, 1, function(x) length(unique(x))))

plotDlmPf <- function(xseq) {
  lower <- apply(xseq, 1, quantile, prob = 0.025)
  upper <- apply(xseq, 1, quantile, prob = 0.975)
  med <- apply(xseq, 1, median)
  plot(dat$x, type = "n", ylim = range(c(dat$x, lower, upper)))
  points(dat$x)
  lines(med, lty = 1)
  lines(lower, lty = 2)
  lines(upper, lty = 2)
}
plotDlmPf(xseq)
```


To fight for degeneracy, the sequential importance sampling resampling (SISR), the particles gets updated with uniform weight being the goal.
```{r dlmSISR}
dlmSISR <- function(y, sigma, tau, m0, s0, N) {
  xseq <- matrix(0, n, N)
  x <- rnorm(N, m0, s0)
  for (t in 1:n) {
    x <- rnorm(N, x, tau)
    w <- dnorm(y[t], x, sigma)
    xseq[t, ] <- x <- sample(x, size = N, replace = TRUE, prob = w)
  }
  xseq
}
```
We rerun the analysis with SISR with the same data and setting.
```{r dlmSISR-run}
xseq <- dlmSISR(dat$y, sigma, tau, 0, 10, 1000)
summary(apply(xseq, 1, function(x) length(unique(x))))
plotDlmPf(xseq)
```


Another popular algorithm is the auxiliary particle filter algorithm (APF) of Pitt and Shephard (1999a). The APF uses importance sampling similarly to the bootstrap filter but includes an additional ``look-ahead step". At each time point t, the APF calculates first-stage weights $w_{t|t−1}$ for particles from time $t − 1$. These weights are calculated using an estimate of the likelihood of the current data given each particle from the previous time point, $\hat p(y_t \mid x_{t-1})$. Particles with high first-stage weights correspond to values of the latent state at time $t − 1$ that are likely to generate the observed data at time $t$. The estimate $\hat p(y_t | x_{t-1})$ can be approximated by choosing an auxiliary variable $\tilde x_t | x_{t-1}$ and setting $\hat p(y_t | x_{t-1}) = g(y_t|\tilde x_{t|t−1})$. Possible methods for choosing $\tilde x_{t | t - 1}$ include simulating a value from $f(x_t|x_{t−1})$ or taking $\tilde x_{t | t - 1} = \E (x_t | x_{t−1})$.
```{r dlmAPF}
dlmAPF <- function(y, sigma, tau, m0, s0, N) {
  xseq <- matrix(0, n, N)
  x <- rnorm(N, m0, s0)
  for (t in 1:n) {
    w0 <- dnorm(y[t], x, sigma)
    tilx <- sample(x, size = N, replace = TRUE, prob = w0)
    x <- rnorm(N, tilx, tau)
    w <- dnorm(y[t], x, sigma) / dnorm(y[t], tilx, sigma)
    xseq[t, ] <- x <- sample(x, size = N, replace = TRUE, prob = w)
  }
  xseq
}

xseq <- dlmAPF(dat$y, sigma, tau, 0, 10, 1000)
summary(apply(xseq, 1, function(x) length(unique(x))))
plotDlmPf(xseq)
```


## Variance Reduction

### Importance Sampling

An integral $\int_a^b H(x) \dd x$ is expressed as
\[
  \mu = \int_a^b h(x) f(x) \dd x
\]
where $f(x)$ is a density with support $(a, b)$ and $h(x) = H(x) / f(x)$.
An ordinary MC estimator of $\mu$ is
\[
  \hat\mu = \frac{1}{n} \sum_{i=1}^n h(Z_i)
\]
where $Z_1, \ldots, Z_n$ are a random sample from $f$.


An importance sampling estimator is 
\[
  \hat\mu = \frac{1}{n} \sum_{i=1}^n \frac{h(X_i)}{g(X_i)}
\]
where $X_1, \ldots, X_n$ are a random sample from a density $g$ whose support is the same as $f$. This estimator is unbiased with $\E(\hat\mu) = \mu$, the optimal choice of $g$ should minimize the variance $\hat\mu$ or equivalently the second moment of $\hat\mu$. From Jensen's inequality
\[
  \E (\hat\mu^2) \ge \E^2 (\hat\mu) = \mu^2.
\]
When the equality holds, we have $\Var(\hat\mu) = 0$. That is the density $g$ should be chosen such that $g(x) \propto |h(x)| f(x) = |H(x)|$. The result is slightly irrelevant since $\int_a^b H(x) \dif x$ is exactly what we need to find out the first place. Nonetheless, it implies that the variance of the weighted estimator is lower if $g(z)$ resembles $|h(z)| f(z)$, in which case, random points are sampled where they are needed most for accuracy.


For illustration, Consider MC integration of $\E[ h(X)]$, where
$h(x) = x^\alpha$ and $X$ is an exponential random variable with mean $\beta$.
The optimal sampler $g$ should be $\Gamma(\alpha + 1, \beta)$.
```{r impExp}
isAppr <- function(n, h, df, dg, rg, ...) {
  x <- rg(n, ...)
  mean( h(x) * df(x) / dg(x, ...) )
}

alpha <- 3
h <- function(x) x^alpha
beta <- 2
df <- function(x) dexp(x, rate = 1 / beta)

mySummary <- function(nrep, n, h, df, dg, rg) {
##    browser()
    sim <- replicate(nrep, isAppr(n, h, df, dg, rg))
    c(mean = mean(sim), sd = sd(sim))
}

sapply(1:6, 
       function(shp) {
           rg <- function(n) rgamma(n, shape = shp, scale = beta)
           dg <- function(x) dgamma(x, shape = shp, scale = beta)
           mySummary(100, 1000, h, df, dg, rg)
       })
```


### Control Variates


## Exercises

1. Suppose $X$ has the following probability density function
\[
f(x) = \frac{1}{5\sqrt{2\pi}}x^2 e^{-\frac{(x-2)^2}{2}}, \qquad -\infty <x < \infty.
\]
Consider using the importance sampling method to estimate $\E(X^2)$.
    a. Implement the important sampling method, with $g(x)$ being the standard
normal density. Report your estimates using 1000, 10000 and 50000
samples. Also estimate the variances of the estimates.
    a. Design a better importance sampling method to estimate $\E(X^2)$ using a
different $g(x)$. Justify your choice of $g(x)$.
    a. Implement your method and estimate $\E(X^2)$ using using 1000, 10000 and 50000
samples. Also estimate the variances of the importance sampling estimates.
    a. Compare the two results from the two methods and comment.
1. Consider a geometric Brownian motion
\begin{align*}
  \frac{\dd S(t)}{S(t)} = r\,\dd t + \sigma\,\dd W(t).
\end{align*}
Let $P_A = e^{-rT}(S_A-K)_+$, $P_E = e^{-rT}[S(T)-K]_+$, and $P_G =
e^{-rT}[S_G-K]_+$, where
\begin{align*}
  S_A = \frac{1}{n} \sum_{i=1}^n S\left(\frac{iT}{n}\right),
  \quad
  S_G = \left[\prod_{i=1}^n S\left(\frac{iT}{n}\right)\right]^{1/n}.
\end{align*}
In all the questions below, $S(0)=1$, $r=0.05$, and $n=12$.
    a. Write down and implement an algorithm to sample the path of $S(t)$.
    a. Set $\sigma=0.5$ and $T=1$.  For each of the values of 
$K \in \{1.1, 1.2, 1.3, 1.4, 1.5\}$, simulate 5000 sample paths of $S(t)$ to get
MC estimates of the correlation coefficients between $P_A$ and
$S(T)$, between $P_A$ and $P_E$, and between $P_A$ and $P_G$.  How
do the correlation coefficients change as $K$ increases?
    a. Set $T=1$ and $K=1.5$.  For each of the values of 
$\sigma = \in \{0.2, 0.3, 0.4, 0.5\}$, simulate 5000 sample paths of $S(t)$ to
get MC estimates of the correlation coefficients.  How do the correlation
coefficients change as $\sigma$ increases?
    a. Set $\sigma=0.5$ and $K=1.5$.  For each of the values of 
$T \in \{0.4, 0.7, 1, 1.3, 1.6\}$, use 5000 sample paths of $S(t)$ to get MC
estimates of the correlation coefficients.  How do the correlation
coefficients change as $T$ increases?
    a. Set $\sigma=0.4$, $T=1$ and $K=1.5$.  Use $P_G$ as a control
variate to develop a control variate MC estimator for $\E(P_A)$.
Compare its SD with the SD of the MC estimator for $\E(P_A)$ that
has no control variate.
